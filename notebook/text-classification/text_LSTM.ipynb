{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a GPU available: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Is there a GPU available: \"),\n",
    "print(tf.config.experimental.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "print(gpus, cpus)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#   try:\n",
    "#     tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#     tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "#   except RuntimeError as e:\n",
    "#     # Visible devices must be set at program startup\n",
    "#     print(e)\n",
    "\n",
    "if cpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(cpus[0], 'CPU')\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set at program startup\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/rt-polaritydata/rt-polarity-pos.txt\n",
      "./data/rt-polaritydata/rt-polarity-neg.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "base_path = './data'\n",
    "raw_data_path = 'rt-polaritydata'\n",
    "\n",
    "parent_dir = os.path.join(base_path, raw_data_path)\n",
    "FILE_NAMES = ['rt-polarity-pos.txt', 'rt-polarity-neg.txt']\n",
    "\n",
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)\n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    text_dir = os.path.join(parent_dir, file_name)\n",
    "    print(text_dir)\n",
    "    \n",
    "    lines_dataset = tf.data.TextLineDataset(text_dir)\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)\n",
    "\n",
    "labeled_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=53, shape=(), dtype=string, numpy=b'compelling revenge thriller , though somewhat weakened by a miscast leading lady . '>, <tf.Tensor: id=54, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=55, shape=(), dtype=string, numpy=b'a pointed , often tender , examination of the pros and cons of unconditional love and familial duties . '>, <tf.Tensor: id=56, shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: id=57, shape=(), dtype=string, numpy=b\"it's tommy's job to clean the peep booths surrounding her , and after viewing this one , you'll feel like mopping up , too . \">, <tf.Tensor: id=58, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=59, shape=(), dtype=string, numpy=b'the movie quickly drags on becoming boring and predictable . i tried to read the time on my watch . '>, <tf.Tensor: id=60, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=61, shape=(), dtype=string, numpy=b'light , silly , photographed with colour and depth , and rather a good time . '>, <tf.Tensor: id=62, shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "\n",
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18369"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "  vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'compelling revenge thriller , though somewhat weakened by a miscast leading lady . '\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2595, 6924, 5196, 8991, 2712, 11275, 17487, 13589, 16725, 5316, 12438]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into test and train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset shapes: ((None, None), (None,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=21634, shape=(42,), dtype=int64, numpy=\n",
       " array([ 2595,  6924,  5196,  8991,  2712, 11275, 17487, 13589, 16725,\n",
       "         5316, 12438,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])>,\n",
       " <tf.Tensor: id=21638, shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One or more dense layers.\n",
    "# Edit the list in the `for` line to experiment with layer sizes.\n",
    "for units in [64, 64]:\n",
    "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1175680   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,254,339\n",
      "Trainable params: 1,254,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "89/89 [==============================] - 6s 68ms/step - loss: 0.7454 - accuracy: 0.5004 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.5899 - accuracy: 0.6727 - val_loss: 0.5364 - val_accuracy: 0.7420\n",
      "Epoch 3/3\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.2653 - accuracy: 0.8937 - val_loss: 0.5873 - val_accuracy: 0.7426\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     79/Unknown - 2s 22ms/step - loss: 0.5491 - accuracy: 0.7486\n",
      "Eval loss: 0.549, Eval accuracy: 0.749\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n",
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hbconfig import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config(\"rt-polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Read config file name: ./config/rt-polarity\n",
       "{\n",
       "    \"data\": {\n",
       "        \"type\": \"rt-polarity\",\n",
       "        \"base_path\": \"data/\",\n",
       "        \"raw_data_path\": \"rt-polaritydata/\",\n",
       "        \"processed_path\": \"rt-polarity_processed_data\",\n",
       "        \"testset_size\": 2000,\n",
       "        \"num_classes\": 2,\n",
       "        \"PAD_ID\": 0\n",
       "    },\n",
       "    \"model\": {\n",
       "        \"batch_size\": 64,\n",
       "        \"embed_type\": \"rand\",\n",
       "        \"pretrained_embed\": \"\",\n",
       "        \"embed_dim\": 300,\n",
       "        \"num_filters\": 256,\n",
       "        \"filter_sizes\": [\n",
       "            2,\n",
       "            3,\n",
       "            4,\n",
       "            5\n",
       "        ],\n",
       "        \"dropout\": 0.5\n",
       "    },\n",
       "    \"train\": {\n",
       "        \"learning_rate\": 1e-05,\n",
       "        \"train_steps\": 20000,\n",
       "        \"model_dir\": \"logs/rt-polarity\",\n",
       "        \"save_checkpoints_steps\": 100,\n",
       "        \"loss_hook_n_iter\": 100,\n",
       "        \"check_hook_n_iter\": 100,\n",
       "        \"min_eval_frequency\": 100,\n",
       "        \"print_verbose\": true,\n",
       "        \"debug\": false\n",
       "    },\n",
       "    \"slack\": {\n",
       "        \"webhook_url\": \"\"\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
